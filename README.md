# ðŸŽ¬ Prompt Evaluation & Tuning System for Generative Video AI

> Analyze and optimize prompts for generative video AI (e.g. Sora, HunyuanVideo, Runway) using CLIP, YOLO, and GPT.

---

## ðŸ“Œ Overview

This project develops a **closed-loop evaluation and tuning system** for generative video AI. It quantifies how well a text prompt is reflected in a generated video and suggests improved prompts using:

- **CLIP-based semantic similarity**
- **YOLO-based object presence detection**
- **GPT-based feedback and rewriting**

---

## ðŸ§  Motivation

Generative video models often fail to fully reflect user intent, and current evaluation relies heavily on subjective judgment.

We aim to:

- Provide a **quantitative measure** of prompt-video alignment  
- Detect whether **key objects** in the prompt appear in the video  
- Offer **automated improvements** using LLM-based feedback  

---

## âš™ï¸ System Architecture

```mermaid
flowchart LR
    A["Prompt Input"] --> B["Video Generation"]
    B --> C["Frame Extraction (OpenCV)"]
    C --> D["CLIP Similarity Evaluation"]
    C --> E["YOLOv8 Object Detection"]
    D --> F["GPT Feedback & Prompt Tuning"]
    E --> F
    F --> G["Improved Prompt Suggestion"]
```

---

## ðŸ’¾ Modules

### 1. CLIP-Based Similarity Analysis
- Use **CLIP ViT-B/32** to compute frame-wise prompt similarity
- Analyze **semantic consistency over time** with line plots
![image](https://github.com/user-attachments/assets/0a782511-e075-4f94-9651-c99b9d39ebbf)


### 2. Object Detection via YOLOv8
- Extract key nouns from the prompt (e.g. "dog", "forest")
- Use **YOLOv8m** to detect object presence in each frame
- ![image](https://github.com/user-attachments/assets/0efb129d-a8b7-4c0b-bbd6-c86abd5deaf9)


### 3. GPT-Based Feedback & Prompt Tuning
- Summarize gaps: missing objects, low similarity areas
- Automatically generate revised prompts with **GPT-4**

### 4. Visualization (Streamlit UI)
- Graphs for CLIP score timeline
- Tables for object appearance summary
- Text output for GPT prompt feedback and suggestions

---

## ðŸ”§ Tech Stack

- **Language**: Python 3.10.11  
- **Libraries**: OpenCV, PyTorch, Huggingface Transformers, Streamlit  
- **Models**: CLIP (ViT-B/32), YOLOv8m, GPT-4  
- **Hardware**: Intel i9-13900, NVIDIA RTX 4080  

---

## ðŸ—‚ Project Structure

```
project-root/
â”œâ”€â”€ input/                     # Raw videos
â”œâ”€â”€ frames/                    # Extracted frame images
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ clip_scores.json
â”‚   â”œâ”€â”€ yolo_objects.json
â”‚   â””â”€â”€ gpt_feedback.txt
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ clip_eval.py
â”‚   â”œâ”€â”€ yolo_detect.py
â”‚   â””â”€â”€ prompt_parser.py
â”œâ”€â”€ streamlit_app.py           # UI Dashboard
â”œâ”€â”€ main.py                    # End-to-end runner
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---


## ðŸ“ˆ Outcomes

- Fully automated **prompt evaluation pipeline**
- Object-aware prompt tuning via GPT
- Compatibility with multiple generative video models
- Useful for **education**, **research**, and **content QA**

---

## ðŸ‘¥ Team

| Name           | Role & Expertise |
|----------------|------------------|
| ê¹€ì£¼í˜• (Juhyeong Kim) | Prompt pipeline, Video AI integration |
| ê¹€ê²½í˜„ (Kyunghyun Kim) | CLIP/YOLO analysis, GPT tuning, Streamlit UI |

---

## ðŸ–¼ Screenshots

> *(![image](https://github.com/user-attachments/assets/d5bbd8c0-ce26-4323-ab5f-31f5a14ea04f)
![image](https://github.com/user-attachments/assets/c057c358-1610-4b98-a4c5-00849b62a3bb))*

---

## ðŸ“š References

For full academic citations, see [`docs/references.md`](./docs/references.md)

---

## ðŸ“œ License

This project is licensed under the **MIT License**. Feel free to use, modify, and distribute it.

---

